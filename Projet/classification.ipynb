{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification : a24_clas_app.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement du jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "setwd(\"C:/Users/antoi/Desktop/UTC/GI05/SY19/Projet\")\n",
    "data <- read.csv(\"a24_clas_app.txt\", sep = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Nombre de données : 500\"\n",
      "[1] \"Nombre de prédicteurs : 50\"\n"
     ]
    }
   ],
   "source": [
    "n <- nrow(data)\n",
    "p <- ncol(data) - 1\n",
    "\n",
    "print(paste(\"Nombre de données :\", n))\n",
    "print(paste(\"Nombre de prédicteurs :\", p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"unable to access index for repository https://cran.r-project.org/bin/windows/contrib/3.6:\n",
      "  impossible d'ouvrir l'URL 'https://cran.r-project.org/bin/windows/contrib/3.6/PACKAGES'\"installing the source package 'corrplot'\n",
      "\n",
      "corrplot 0.95 loaded\n"
     ]
    }
   ],
   "source": [
    "install.packages(\"corrplot\")\n",
    "library(\"corrplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in library(Matrix): there is no package called 'Matrix'\n",
     "output_type": "error",
     "traceback": [
      "Error in library(Matrix): there is no package called 'Matrix'\nTraceback:\n",
      "1. library(Matrix)"
     ]
    }
   ],
   "source": [
    "library(Matrix)\n",
    "X <- as.matrix(data[, 1:p])\n",
    "rankMatrix(X)\n",
    "\n",
    "library(\"corrplot\")\n",
    "cor_data = cor(data)\n",
    "cor_long <- as.data.frame(as.table(cor_data))\n",
    "cor_long <- cor_long[cor_long$Var1 != cor_long$Var2, ]\n",
    "cor_long <- cor_long[as.numeric(as.factor(cor_long$Var1)) < as.numeric(as.factor(cor_long$Var2)), ]\n",
    "cor_long$abs_value <- abs(cor_long$Freq)\n",
    "cor_long <- cor_long[order(-cor_long$abs_value), ]\n",
    "top_10_correlations <- head(cor_long, 10)\n",
    "print(top_10_correlations)\n",
    "\n",
    "# Séparation en jeu d'entrainement de et test\n",
    "train <- sample(1:n, round(4*n/5))\n",
    "data.train <- data[train, ]\n",
    "data.test <- data[-train, ]\n",
    "\n",
    "# Observation de la distribution de certaines variables\n",
    "boxplot(data$X1, data$X2, data$X3,\n",
    "        names = c(\"X1\", \"X2\", \"X3\"),\n",
    "        main = \"Boxplot of X1, X2, and X3\", # Title\n",
    "        ylab = \"Values\", # Y-axis label\n",
    "        col = c(\"lightblue\", \"lightgreen\", \"lightcoral\"))\n",
    "\n",
    "plot(data$X2 ~ data$X1, main = \"Valeurs de X2 en fonction de X1\")\n",
    "\n",
    "barplot(data$X1, main = \"Diagramme en barre des valeurs de X1\")\n",
    "\n",
    "barplot(table(data$y), main = \"Distribution des valeurs de y\", xlab = \"y\", ylab = \"Nombre\", col = \"lightblue\", border = \"black\")\n",
    "\n",
    "# KNN\n",
    "\n",
    "#install.packages(\"e1071\") \n",
    "#install.packages(\"caTools\") \n",
    "#install.packages(\"class\") \n",
    "\n",
    "library(e1071) \n",
    "library(caTools) \n",
    "library(class) \n",
    "\n",
    "# Normalisation des données\n",
    "data.train.x_scale <- scale(data.train[, 1:50])\n",
    "data.test.x_scale <- scale(data.test[, 1:50], center = attr(data.train.x_scale, \"scaled:center\"), scale = attr(data.train.x_scale, \"scaled:scale\"))\n",
    "\n",
    "K <- 10\n",
    "folds <- sample(1:K, nrow(data.train), replace = TRUE)\n",
    "table(folds)\n",
    "accuracy.train <- rep(0, 100)\n",
    "accuracy.val <- rep(0, 100)\n",
    "\n",
    "# Test des différentes valeurs de k\n",
    "for (k in 1:100){  \n",
    "  accuracy.train.fold <- rep(0, K)\n",
    "  accuracy.val.fold <- rep(0, K)\n",
    "  \n",
    "  # Pour chaque pli\n",
    "  for (fold in 1:K){\n",
    "    # Données d'entraînement et de validation\n",
    "    x_train_data <- data.train.x_scale[folds != fold, ]\n",
    "    y_train_data <- data.train[folds != fold, ]$y\n",
    "    x_validation_data <- data.train.x_scale[folds == fold, ]\n",
    "    y_validation_data <- data.train[folds == fold, ]$y\n",
    "    \n",
    "    # Application du k-NN pour les données de validation\n",
    "    classifier_knn_val <- knn(train = x_train_data, test = x_validation_data, cl = y_train_data, k = k)\n",
    "    # Calcul de l'accuracy pour ce pli sur les données de validation\n",
    "    accuracy.val.fold[fold] <- sum(classifier_knn_val == y_validation_data) / length(y_validation_data)\n",
    "    \n",
    "    # Application du k-NN pour les données d'entraînement\n",
    "    classifier_knn_train <- knn(train = x_train_data, test = x_train_data, cl = y_train_data, k = k)\n",
    "    # Calcul de l'accuracy pour ce pli sur les données d'entraînement\n",
    "    accuracy.train.fold[fold] <- sum(classifier_knn_train == y_train_data) / length(y_train_data)\n",
    "  }\n",
    "  # Moyenne des accuracies sur tous les plis\n",
    "  accuracy.val[k] <- mean(accuracy.val.fold, na.rm = TRUE)\n",
    "  accuracy.train[k] <- mean(accuracy.train.fold, na.rm = TRUE)\n",
    "}\n",
    "\n",
    "plot(1:100, accuracy.train, main = \"Accuracy sur les données d'entrainement \\net de validation avec KNN\",\n",
    "     col = \"blue\", type = \"l\", xlab = \"Nombre de voisins (k)\", ylab = \"Accuracy\")\n",
    "lines(1:100, accuracy.val, col = \"green\")\n",
    "max_accuracy_val <- max(accuracy.val)\n",
    "max_index <- which.max(accuracy.val)\n",
    "abline(v = max_index, col = \"red\", lty = 2)\n",
    "legend(\"topright\", legend = c(\"Entraînement\", \"Validation\"), col = c(\"blue\", \"green\"), lty = 1)\n",
    "sprintf(\"Le nombre de voisins optimal est : %d\", max_index)\n",
    "\n",
    "\n",
    "classifier_knn <- knn(train = data.train[, 1:50], test = data.test[, 1:50], cl = data.train$y, k = 26)\n",
    "knn.accuracy.test <- sum(classifier_knn == data.test$y) / length(data.test$y)\n",
    "print(knn.accuracy.test) # Renvoie 0.46\n",
    "\n",
    "\n",
    "# Régression logistique classique\n",
    "\n",
    "library(nnet)\n",
    "reg_log <- multinom(y ~ ., data = data.train)\n",
    "reg_log.pred <- predict(reg_log, newdata=data.test[, 1:50],type='class')\n",
    "reg_log.confusion_matrix <- table(Predicted = reg_log.pred, Actual = data.test$y)\n",
    "reg_log.confusion_matrix\n",
    "reg_log.accuracy.test <- sum(reg_log.pred == data.test$y) / length(data.test$y) \n",
    "print(reg_log.accuracy.test) # Renvoie 0.62\n",
    "\n",
    "# QDA \n",
    "\n",
    "library(MASS)\n",
    "qda <- qda(y ~ ., data = data.train)\n",
    "qda.pred <- predict(qda, newdata = data.test)$class\n",
    "qda.confusion_matrix <- table(Predicted = qda.pred, Actual = data.test$y)\n",
    "qda.confusion_matrix\n",
    "qda.accuracy.test <- sum(qda.pred == data.test$y) / length(data.test$y) \n",
    "print(qda.accuracy.test) # Renvoie 0.64\n",
    "\n",
    "# LDA\n",
    "\n",
    "lda <- lda(y ~ ., data = data.train)\n",
    "lda.pred <- predict(lda, newdata = data.test)$class\n",
    "lda.confusion_matrix <- table(Predicted = lda.pred, Actual = data.test$y)\n",
    "lda.confusion_matrix\n",
    "lda.accuracy.test <- sum(lda.pred == data.test$y) / length(data.test$y) \n",
    "print(lda.accuracy.test) # Renvoie 0.62\n",
    "\n",
    "# Bayes Naîf\n",
    "\n",
    "naive_bayes <- naiveBayes(y ~ ., data = data.train)\n",
    "naive_bayes.pred <- predict(naive_bayes, newdata = data.test)\n",
    "naive_bayes.confusion_matrix <- table(Predicted = naive_bayes.pred, Actual = data.test$y)\n",
    "naive_bayes.confusion_matrix\n",
    "naive_bayes.accuracy.test <- sum(naive_bayes.pred == data.test$y) / length(data.test$y) \n",
    "print(naive_bayes.accuracy.test) # Renvoie 0.7\n",
    "\n",
    "# Test de McNemar\n",
    "\n",
    "library(stats)\n",
    "mcnemar.test(lda.pred == data.test$y, qda.pred == data.test$y)\n",
    "\n",
    "# SVM radial\n",
    "\n",
    "data.train_scaled <- data.frame(y = data.train$y, data.train.x_scale)\n",
    "data.test_scaled <- data.frame(y = data.test$y, data.test.x_scale)\n",
    "\n",
    "svm_model <- svm(y ~ ., data = data.train_scaled, type = \"C-classification\", kernel = \"radial\", cross = 10)\n",
    "svm.pred <- predict(svm_model, newdata = data.test_scaled)\n",
    "svm.confusion_matrix <- table(Predicted = svm.pred, Actual = data.test_scaled$y)\n",
    "print(svm.confusion_matrix)\n",
    "svm.accuracy.test <- sum(svm.pred == data.test_scaled$y) / length(data.test_scaled$y)\n",
    "print(svm.accuracy.test) #0.63\n",
    "\n",
    "# SVM linear\n",
    "\n",
    "data.train_scaled <- data.frame(y = data.train$y, data.train.x_scale)\n",
    "data.test_scaled <- data.frame(data.test.x_scale, y = data.test$y)\n",
    "\n",
    "svm_model <- svm(y ~ ., data = data.train_scaled, type = \"C-classification\", kernel = \"linear\", cross = 10)\n",
    "svm.pred <- predict(svm_model, newdata = data.test_scaled)\n",
    "svm.confusion_matrix <- table(Predicted = svm.pred, Actual = data.test_scaled$y)\n",
    "print(svm.confusion_matrix)\n",
    "svm.accuracy.test <- sum(svm.pred == data.test_scaled$y) / length(data.test_scaled$y)\n",
    "print(svm.accuracy.test) #0.55\n",
    "\n",
    "\n",
    "# Tree\n",
    "library(rpart)\n",
    "library(rpart.plot)\n",
    "tree_model <- rpart(y~., data = data.train, method = \"class\", parms = list(split = 'gini'))\n",
    "rpart.plot(tree_model, box.palette=\"RdBu\", shadow.col=\"gray\",\n",
    "           fallen.leaves=FALSE)\n",
    "plotcp(tree_model)\n",
    "\n",
    "\n",
    "# Random Forest\n",
    "library(randomForest)\n",
    "\n",
    "x.train <- data.train_scaled[, -1]  # Caractéristiques d'entraînement sans la colonne cible\n",
    "y.train <- data.train_scaled$y      # Variable cible d'entraînement\n",
    "x.test <- data.test_scaled[, -1]    # Caractéristiques de test sans la colonne cible\n",
    "y.test <- data.test_scaled$y        # Variable cible de test\n",
    "\n",
    "y.train <- as.factor(y.train)\n",
    "y.test <- as.factor(y.test)\n",
    "\n",
    "rf_model <- randomForest(x = x.train, y = y.train, \n",
    "                         xtest = x.test, ytest = y.test,\n",
    "                         ntree = 500, \n",
    "                         mtry = floor(sqrt(ncol(x.train))),\n",
    "                         nodesize = 1,\n",
    "                         importance = TRUE, \n",
    "                         keep.forest = TRUE)\n",
    "\n",
    "print(rf_model)\n",
    "rf.confusion_matrix <- table(Predicted = rf_model$test$predicted, Actual = y.test)\n",
    "print(rf.confusion_matrix)\n",
    "\n",
    "rf.accuracy.test <- sum(rf_model$test$predicted == y.test) / length(y.test)\n",
    "print(rf.accuracy.test) #0.58\n",
    "\n",
    "# Importance des variables\n",
    "\n",
    "varImpPlot(rf_model)\n",
    "\n",
    "\n",
    "# ACP\n",
    "data_standardized <- scale(data[, !names(data) %in% 'y'])\n",
    "acp_result <- prcomp(data_standardized, center = TRUE, scale. = TRUE)\n",
    "summary(acp_result)\n",
    "\n",
    "# Visualisation des composantes principales (biplot)\n",
    "biplot(acp_result, scale = 1)\n",
    "\n",
    "# Tracé de la variance expliquée par chaque composante principale\n",
    "screeplot(acp_result, type = \"lines\", main = \"Scree Plot des 20 premiers axes\", npcs = 20)\n",
    "\n",
    "# Diagramme des variances expliquées cumulées\n",
    "explained_variance <- cumsum(acp_result$sdev^2 / sum(acp_result$sdev^2)) * 100\n",
    "plot(1:length(explained_variance), explained_variance, col = \"blue\", type = \"l\", \n",
    "     xlab = \"Nombre de composantes principales\", ylab = \"Variance expliquée cumulée (%)\",\n",
    "     main = \"Variance expliquée cumulée par les composantes principales\")\n",
    "\n",
    "# Apprentissage sur ces données PCA\n",
    "n <- nrow(data)  \n",
    "p <- ncol(data) - 1\n",
    "\n",
    "train <- sample(1:n, round(4 * n / 5))\n",
    "test <- setdiff(1:n, train)\n",
    "\n",
    "acp_results <- acp_result$x[, 1:p]\n",
    "data_pca <- data.frame(acp_results, y = data$y)\n",
    "\n",
    "train_data_pca <- data_pca[train, ]\n",
    "test_data_pca <- data_pca[test, ]\n",
    "\n",
    "K <- 5  \n",
    "fold <- sample(1:K, length(train), replace = TRUE)\n",
    "table(fold)\n",
    "\n",
    "mean_accuracy <- rep(0, p)\n",
    "\n",
    "for (nb_axes in 1:p) {\n",
    "  accuracy <- rep(0, K)\n",
    "  for (k in 1:K) {\n",
    "    glm_pca <- multinom(y ~ ., data = train_data_pca[fold != k, c(1:nb_axes, ncol(train_data_pca))])\n",
    "    pred <- predict(glm_pca, newdata = train_data_pca[fold == k, c(1:nb_axes, ncol(train_data_pca))], type = \"class\")\n",
    "    accuracy[k] <- sum(pred == data.test_scaled$y) / length(data.test_scaled$y)\n",
    "  }\n",
    "  mean_accuracy[nb_axes] <- mean(accuracy)\n",
    "}\n",
    "\n",
    "# Nombre optimal de composantes principales\n",
    "best_nb_axes <- which.max(mean_accuracy)\n",
    "cat(sprintf(\"Le nombre optimal de composantes principales est : %i\\n\", best_nb_axes))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
