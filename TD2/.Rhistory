prostate <- read.csv("prostate.data", sep = "\t")
prostate
# Affichage pour voir quelles variables contribuent à lpsa
plot(lpsa ~lcavol, data = prostate) # Oui, lien linéaire
plot(lpsa ~lweight, data = prostate)
plot(lpsa ~age, data = prostate)
plot(lpsa ~lbph, data = prostate)
boxplot(lpsa ~svi, data = prostate) # Oui
plot(lpsa ~lcp, data = prostate)
boxplot(lpsa ~gleason, data = prostate)
plot(lpsa ~pgg45, data = prostate)
library(FNN)
train_data = prostate[prostate$train, c("lcavol", "lweight", "age", "lbph", "lpsa")]
train_data_X = train_data[c("lcavol", "lweight", "age", "lbph")]
train_data_y = train_data$lpsa
test_data = prostate[!prostate$train, c("lcavol", "lweight", "age", "lbph", "lpsa")]
test_data_X = test_data[c("lcavol", "lweight", "age", "lbph")]
test_data_y = test_data$lpsa
knn.reg(train = train_data_X, test = test_data_X, y = train_data_y)$pred
# Représentation graphique du MSE sur l'ensemble d'apprentissage
mse = function(actual, predicted) {
mean((actual - predicted) ^ 2)
}
liste_voisins <- 1:20
liste_erreurs_train <- c()
for (nb_voisins in liste_voisins) {
knn_values <- knn.reg(train = train_data_X, test = test_data_X, y = train_data_y, k = nb_voisins)
MSE = mse(test_data_y, knn_values$pred)
liste_erreurs_train <- c(liste_erreurs_train, MSE)
}
plot(liste_erreurs_train ~liste_voisins, type = "l")
# le minimum est atteint en k = 2
# Etude du compromis biais-variance
n = 50
X <- runif(n)
epsilon <- rnorm(n, mean = 0, sd = 0.5)
Y = 1 + 5*X**2 + epsilon
n = 50
X <- runif(n)
epsilon <- rnorm(n, mean = 0, sd = 0.5)
Y = 1 + 5*X**2 + epsilon
plot(X, Y)
help rep
rep help
help
plot(1:Kmax,error,type="l",
ylim=range(error,biais2,variance),
xlab="k", ylab="MSE",lwd=2,col="blue")
n = 50
X <- runif(n)
epsilon <- rnorm(n, mean = 0, sd = 0.5)
Y = 1 + 5*X**2 + epsilon
plot(X, Y)
x0<-0.5
Ey0<-1+5*x0^2  # Valeur en x0 de la fct de régression
Kmax<-40  # Valeur max de k
N<-10000 # nombre d'ensembles d'apprentissage
yhat<-matrix(0,N,Kmax)
y0<-rep(0,N)
for(i in 1:N){
x<-runif(n)
y<-1+5*x^2+sig*rnorm(n)
d<-abs(x-x0)
ds<-sort(d,index.return=TRUE) # tri des distances à x0
y0[i]<-Ey0+sig*rnorm(1) # génération de Y en x0
for(K in 1:Kmax) yhat[i,K]<-mean(y[ds$ix[1:K]]) # prédictions
}
sig = 0.5
x0<-0.5
Ey0<-1+5*x0^2  # Valeur en x0 de la fct de régression
Kmax<-40  # Valeur max de k
N<-10000 # nombre d'ensembles d'apprentissage
yhat<-matrix(0,N,Kmax)
y0<-rep(0,N)
for(i in 1:N){
x<-runif(n)
y<-1+5*x^2+sig*rnorm(n)
d<-abs(x-x0)
ds<-sort(d,index.return=TRUE) # tri des distances à x0
y0[i]<-Ey0+sig*rnorm(1) # génération de Y en x0
for(K in 1:Kmax) yhat[i,K]<-mean(y[ds$ix[1:K]]) # prédictions
}
error<-rep(0,K)
biais2<-rep(0,K)
variance<-rep(0,K)
for(K in 1:Kmax){
error[K]<-mean((yhat[,K]-y0)^2)   # MSE
biais2[K]<-(mean(yhat[,K])-Ey0)^2 # biais^2
variance[K]<-var(yhat[,K])        # variance
}
plot(1:Kmax,error,type="l",
ylim=range(error,biais2,variance),
xlab="k", ylab="MSE",lwd=2,col="blue")
lines(1:Kmax,biais2,lty=2,lwd=2)
lines(1:Kmax,variance,lty=3,lwd=2)
lines(1:Kmax,biais2+variance+sig^2,col="red",lwd=2)
prostate <- read.csv("prostate.data", sep = "\t")
prostate
fit <- lm(lpsa ~ ., data = prostate)
fit
summary(lm)
summary(fit)
prostate
prostate <- subset(prosate, select = -X)
prostate <- subset(prostate, select = -X)
prostate
fit <- lm(lpsa ~ ., data = prostate)
summary(fit)
fit$coefficients
fit$fitted.values
fit$coefficients
fit$residuals
fit$coefficients
View(fit)
summary(fit)$coefficients["Estimate", "Std. Error"]
summary(fit)$coefficients[, c("Estimate", "Std. Error")]
confint(fit)
# Affichage des yi prédits en fonction des yi
plot(fit$fitted.values ~ prostate$lpsa)
plot(fit$fitted.values ~ prostate$lpsa, xlab = "y", ylab = "Valeurs y prédites")
plot(fit$fitted.values ~ prostate$lpsa, xlab = "Valeurs y", ylab = "Valeurs y prédites", title = "Test")
plot(fit$fitted.values ~ prostate$lpsa, xlab = "Valeurs y", ylab = "Valeurs y prédites", main = "Valeurs prédites en fonction des valeurs réelles")
# Affichage des yi prédits en fonction des yi
plot(fit$fitted.values ~ prostate$lpsa, xlab = "Valeurs y", ylab = "Valeurs y prédites", main = "Valeurs prédites en fonction \n des valeurs réelles")
train_data <- prostate[prostate$train]
train_data <- prostate[prostate$train, ]
train_data <- prostate[prostate$train, c("lcavol", "lweight", "age", "lbph", "lpsa")]
fit <- lm(lpsa ~ ., data = prostate)
test_data <- prostate[~prostate$train, c("lcavol", "lweight", "age", "lbph", "lpsa")]
test_data <- prostate[!prostate$train, c("lcavol", "lweight", "age", "lbph", "lpsa")]
prostate <- prostate[, c("lcavol", "lweight", "age", "lbph", "lpsa")]
train_data <- prostate[prostate$train, ]
test_data <- prostate[!prostate$train, ]
prostate <- prostate[, c("train", "lcavol", "lweight", "age", "lbph", "lpsa")]
prostate <- read.csv("prostate.data", sep = "\t")
prostate <- subset(prostate, select = -X)
prostate <- prostate[, c("train", "lcavol", "lweight", "age", "lbph", "lpsa")]
train_data <- prostate[prostate$train, c("lcavol", "lweight", "age", "lbph", "lpsa")]
test_data <- prostate[!prostate$train, c("lcavol", "lweight", "age", "lbph", "lpsa")]
fit <- lm(lpsa ~ ., data = prostate, subset = train_data)
prostate <- read.csv("prostate.data", sep = "\t")
prostate <- subset(prostate, select = -X)
train_data <- prostate[prostate$train, c("lcavol", "lweight", "age", "lbph", "lpsa")]
test_data <- prostate[!prostate$train, c("lcavol", "lweight", "age", "lbph", "lpsa")]
prostate <- prostate[, c("lcavol", "lweight", "age", "lbph", "lpsa")]
fit <- lm(lpsa ~ ., data = prostate, subset = train_data)
# Chargement des données de prostate
prostate <- read.csv("prostate.data", sep = "\t")
prostate <- subset(prostate, select = -X)
# Régression linéaire
fit <- lm(lpsa ~ ., data = prostate)
summary(fit)
# Calcul des intervalles de confiance sur les coefficients
confint(fit)
# Affichage des yi prédits en fonction des yi
plot(fit$fitted.values ~ prostate$lpsa, xlab = "Valeurs y", ylab = "Valeurs y prédites", main = "Valeurs prédites en fonction \n des valeurs réelles")
# Apprentissage avec jeu d'apprentissage
train_data <- prostate[prostate$train, c("lcavol", "lweight", "age", "lbph", "lpsa")]
test_data <- prostate[!prostate$train, c("lcavol", "lweight", "age", "lbph", "lpsa")]
fit <- lm(lpsa ~ ., data = train_data)
p <- predict(fit, test_data)
p
mse <- mean((test_data$lpsa - p)**2)
mse
# Chargement des données de prostate
prostate <- read.csv("prostate.data", sep = "\t")
prostate
# Affichage pour voir quelles variables contribuent à lpsa
plot(lpsa ~lcavol, data = prostate) # Oui, lien linéaire
plot(lpsa ~lweight, data = prostate)
plot(lpsa ~age, data = prostate)
plot(lpsa ~lbph, data = prostate)
boxplot(lpsa ~svi, data = prostate) # Oui
plot(lpsa ~lcp, data = prostate)
boxplot(lpsa ~gleason, data = prostate)
plot(lpsa ~pgg45, data = prostate)
# Installation du package FNN
#install.packages("FNN")
library(FNN)
train_data = prostate[prostate$train, c("lcavol", "lweight", "age", "lbph", "lpsa")]
train_data_X = train_data[c("lcavol", "lweight", "age", "lbph")]
train_data_y = train_data$lpsa
test_data = prostate[!prostate$train, c("lcavol", "lweight", "age", "lbph", "lpsa")]
test_data_X = test_data[c("lcavol", "lweight", "age", "lbph")]
test_data_y = test_data$lpsa
knn.reg(train = train_data_X, test = test_data_X, y = train_data_y)$pred
# Représentation graphique du MSE sur l'ensemble d'apprentissage
mse = function(actual, predicted) {
mean((actual - predicted) ^ 2)
}
liste_voisins <- 1:20
liste_erreurs_train <- c()
for (nb_voisins in liste_voisins) {
knn_values <- knn.reg(train = train_data_X, test = test_data_X, y = train_data_y, k = nb_voisins)
MSE = mse(test_data_y, knn_values$pred)
liste_erreurs_train <- c(liste_erreurs_train, MSE)
}
plot(liste_erreurs_train ~liste_voisins, type = "l")
# le minimum est atteint en k = 2
plot(p ~ test_data$lpsa)
plot(p ~ test_data$lpsa, xlab = "Valeurs réelles", ylab = "Valeurs prédites", main = "Prostate dataset")
CIs <- confint.default(fit, level = 0.95)
xlim <- c(min(CIs), max(CIs))
par(mar = c(5, 7, 3, 1) + 0.1)
plotCI(CIs, main = "95% Confidence Intervals", xlim = xlim, cex.y = 0.9,
xlab = "Beta Coefficients")
library(gplots)
install.packages("gplots")
library(gplots)
CIs <- confint.default(fit, level = 0.95)
xlim <- c(min(CIs), max(CIs))
par(mar = c(5, 7, 3, 1) + 0.1)
plotCI(CIs, main = "95% Confidence Intervals", xlim = xlim, cex.y = 0.9,
xlab = "Beta Coefficients")
abline(v = 0)
CIs <- confint.default(fit, level = 0.95)
xlim <- c(min(CIs), max(CIs))
plotCI(CIs, main = "95% Confidence Intervals", xlim = xlim, cex.y = 0.9,
xlab = "Beta Coefficients")
abline(v = 0)
p <- predict(fit, test_data, interval = 'confidence')
p
plot(test_data$lpsa, p[, "fit"], main = "Valeurs prédites avec intervalles de confiance",
xlab = "Valeurs réelles", ylab = "Valeurs observées", pch = 20, col = "blue")
arrows(prostate$lpsa, p[, "lwr"], prostate$lpsa, p[, "upr"],
length = 0.05, angle = 90, code = 3, col = "red")
plot(test_data$lpsa, p[, "fit"], main = "Valeurs prédites avec intervalles de confiance",
xlab = "Valeurs réelles", ylab = "Valeurs observées", pch = 20, col = "blue")
arrows(test_data$lpsa, p[, "lwr"], prostate$lpsa, p[, "upr"],
length = 0.05, angle = 90, code = 3, col = "red")
plot(test_data$lpsa, p[, "fit"], main = "Valeurs prédites avec intervalles de confiance",
xlab = "Valeurs réelles", ylab = "Valeurs observées", pch = 20, col = "blue")
arrows(test_data$lpsa, p[, "lwr"], test_data$lpsa, p[, "upr"],
length = 0.05, angle = 90, code = 3, col = "red")
plot(test_data$lpsa, p[, "fit"], main = "Valeurs prédites\navec intervalles de confiance",
xlab = "Valeurs réelles", ylab = "Valeurs observées", pch = 20, col = "blue")
arrows(test_data$lpsa, p[, "lwr"], test_data$lpsa, p[, "upr"],
length = 0.05, angle = 90, code = 3, col = "red")
plot(test_data$lpsa, p[, "fit"], main = "Valeurs prédites\navec intervalles de confiance",
xlab = "Valeurs réelles", ylab = "Valeurs prédites", pch = 20, col = "blue")
arrows(test_data$lpsa, p[, "lwr"], test_data$lpsa, p[, "upr"],
length = 0.05, angle = 90, code = 3, col = "red")
