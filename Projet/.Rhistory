} else if (model == "lasso") {
cv.out <- cv.glmnet(xtrain[fold != k, ], ytrain[fold != k], alpha = 1, standardize = TRUE)
fit <- glmnet(xtrain[fold != k, ], ytrain[fold != k], lambda = cv.out$lambda.min, alpha = 1, standardize = TRUE)
pred <- predict(fit, newx = xtrain[fold == k, ])
} else if (model == "gam") {
fit <- gam(y ~ s(.), data = train_data_pca)
pred <- predict(fit, newdata = val_data_pca)
} else if (model == "elastic_net") {
cv.out <- cv.glmnet(xtrain[fold != k, ], ytrain[fold != k], alpha = elastic_net_alpha, standardize = TRUE)
fit <- glmnet(xtrain[fold != k, ], ytrain[fold != k], lambda = cv.out$lambda.min, alpha = elastic_net_alpha, standardize = TRUE)
pred <- predict(fit, newx = xtrain[fold == k, ])
} else if (model == "svm_linear") {
fit <- svm(y ~ ., data = train_data_pca, type = "eps-regression", kernel = "linear")
pred <- predict(fit, newdata = val_data_pca)
} else if (model == "svm_radial") {
fit <- svm(y ~ ., data = train_data_pca, type = "eps-regression", kernel = "radial")
pred <- predict(fit, newdata = val_data_pca)
} else if (model == "knn") {
pred <- knn(train = train_data_pca[, -1], test = val_data_pca[, -1], cl = train_data_pca$y, k = k_neighbors)
} else if (model == "tree") {
# Entraîner un modèle d'arbre de décision
fit <- rpart(y ~ ., data = train_data_pca)
pred <- predict(fit, newdata = val_data_pca)
} else if (model == "random_forest") {
fit <- randomForest(y ~ ., data = train_data_pca)
pred <- predict(fit, newdata = val_data_pca)
}
# Calculate specified metric for the validation fold
metric.val[k] <- calculate_metric(val_data_pca$y, pred, metric)
}
# Return mean and standard deviation of the selected metric across folds
c(mean(metric.val), sd(metric.val))
}
summary_metric_models(data, "RMSE")
library(mgcv)
library(glmnet)
library(caret)
summary_metric_models(data, "RMSE")
estimate_regression_metrics <- function(model, data.train, metric = "RMSE", pca_components = 0, elastic_net_alpha = 0.5, k_neighbors = 15) {
# Number of folds
K <- 10
set.seed(20240811)
# Create fold assignments
fold <- sample(1:K, nrow(data.train), replace = TRUE)
# Initialize metric values
metric.val <- rep(0, K)
# Extract response variable and predictor matrix
ytrain <- data.train$y
xtrain <- model.matrix(y ~ . - 1, data.train)  # Exclude intercept
for (k in 1:K) {
# Split data into training and validation sets
train_data <- data.train[fold != k, ]
val_data <- data.train[fold == k, ]
# If PCA is required, preprocess with PCA on training data only
if (pca_components != 0) {
pca <- prcomp(train_data[, -which(names(train_data) == "y")], center = TRUE, scale. = TRUE)
xtrain_pca <- predict(pca, train_data[, -which(names(train_data) == "y")])
xval_pca <- predict(pca, val_data[, -which(names(val_data) == "y")])
# Select the specified number of components
xtrain_pca <- xtrain_pca[, 1:pca_components]
xval_pca <- xval_pca[, 1:pca_components]
# Update training and validation sets with PCA components
train_data_pca <- data.frame(y = train_data$y, xtrain_pca)
val_data_pca <- data.frame(y = val_data$y, xval_pca)
} else {
train_data_pca <- train_data
val_data_pca <- val_data
}
# Model fitting and prediction
if (model == "lm") {
fit <- lm(y ~ ., data = train_data_pca)
pred <- predict(fit, newdata = val_data_pca)
} else if (model == "ridge") {
cv.out <- cv.glmnet(xtrain[fold != k, ], ytrain[fold != k], alpha = 0, standardize = TRUE)
fit <- glmnet(xtrain[fold != k, ], ytrain[fold != k], lambda = cv.out$lambda.min, alpha = 0, standardize = TRUE)
pred <- predict(fit, newx = xtrain[fold == k, ])
} else if (model == "lasso") {
cv.out <- cv.glmnet(xtrain[fold != k, ], ytrain[fold != k], alpha = 1, standardize = TRUE)
fit <- glmnet(xtrain[fold != k, ], ytrain[fold != k], lambda = cv.out$lambda.min, alpha = 1, standardize = TRUE)
pred <- predict(fit, newx = xtrain[fold == k, ])
} else if (model == "gam") {
fit <- gam(y ~ ., data = train_data_pca)
pred <- predict(fit, newdata = val_data_pca)
} else if (model == "elastic_net") {
cv.out <- cv.glmnet(xtrain[fold != k, ], ytrain[fold != k], alpha = elastic_net_alpha, standardize = TRUE)
fit <- glmnet(xtrain[fold != k, ], ytrain[fold != k], lambda = cv.out$lambda.min, alpha = elastic_net_alpha, standardize = TRUE)
pred <- predict(fit, newx = xtrain[fold == k, ])
} else if (model == "svm_linear") {
fit <- svm(y ~ ., data = train_data_pca, type = "eps-regression", kernel = "linear")
pred <- predict(fit, newdata = val_data_pca)
} else if (model == "svm_radial") {
fit <- svm(y ~ ., data = train_data_pca, type = "eps-regression", kernel = "radial")
pred <- predict(fit, newdata = val_data_pca)
} else if (model == "knn") {
pred <- knn(train = train_data_pca[, -1], test = val_data_pca[, -1], cl = train_data_pca$y, k = k_neighbors)
} else if (model == "tree") {
# Entraîner un modèle d'arbre de décision
fit <- rpart(y ~ ., data = train_data_pca)
pred <- predict(fit, newdata = val_data_pca)
} else if (model == "random_forest") {
fit <- randomForest(y ~ ., data = train_data_pca)
pred <- predict(fit, newdata = val_data_pca)
}
# Calculate specified metric for the validation fold
metric.val[k] <- calculate_metric(val_data_pca$y, pred, metric)
}
# Return mean and standard deviation of the selected metric across folds
c(mean(metric.val), sd(metric.val))
}
summary_metric_models(data, "RMSE")
estimate_regression_metrics <- function(model, data.train, metric = "RMSE", pca_components = 0, elastic_net_alpha = 0.5, k_neighbors = 15) {
# Number of folds
K <- 10
set.seed(20240811)
# Create fold assignments
fold <- sample(1:K, nrow(data.train), replace = TRUE)
# Initialize metric values
metric.val <- rep(0, K)
# Extract response variable and predictor matrix
ytrain <- data.train$y
xtrain <- model.matrix(y ~ . - 1, data.train)  # Exclude intercept
for (k in 1:K) {
# Split data into training and validation sets
train_data <- data.train[fold != k, ]
val_data <- data.train[fold == k, ]
# If PCA is required, preprocess with PCA on training data only
if (pca_components != 0) {
pca <- prcomp(train_data[, -which(names(train_data) == "y")], center = TRUE, scale. = TRUE)
xtrain_pca <- predict(pca, train_data[, -which(names(train_data) == "y")])
xval_pca <- predict(pca, val_data[, -which(names(val_data) == "y")])
# Select the specified number of components
xtrain_pca <- xtrain_pca[, 1:pca_components]
xval_pca <- xval_pca[, 1:pca_components]
# Update training and validation sets with PCA components
train_data_pca <- data.frame(y = train_data$y, xtrain_pca)
val_data_pca <- data.frame(y = val_data$y, xval_pca)
} else {
train_data_pca <- train_data
val_data_pca <- val_data
}
# Model fitting and prediction
if (model == "lm") {
fit <- lm(y ~ ., data = train_data_pca)
pred <- predict(fit, newdata = val_data_pca)
} else if (model == "ridge") {
cv.out <- cv.glmnet(xtrain[fold != k, ], ytrain[fold != k], alpha = 0, standardize = TRUE)
fit <- glmnet(xtrain[fold != k, ], ytrain[fold != k], lambda = cv.out$lambda.min, alpha = 0, standardize = TRUE)
pred <- predict(fit, newx = xtrain[fold == k, ])
} else if (model == "lasso") {
cv.out <- cv.glmnet(xtrain[fold != k, ], ytrain[fold != k], alpha = 1, standardize = TRUE)
fit <- glmnet(xtrain[fold != k, ], ytrain[fold != k], lambda = cv.out$lambda.min, alpha = 1, standardize = TRUE)
pred <- predict(fit, newx = xtrain[fold == k, ])
} else if (model == "gam") {
predictor_names <- names(train_data_pca)[-which(names(train_data_pca) == "y")]
formula_str <- paste("y ~", paste("s(", predictor_names, ")", collapse = " + "))
fit <- gam(as.formula(formula_str), data = train_data_pca, family = gaussian())
pred <- predict(fit, newdata = val_data_pca)
} else if (model == "elastic_net") {
cv.out <- cv.glmnet(xtrain[fold != k, ], ytrain[fold != k], alpha = elastic_net_alpha, standardize = TRUE)
fit <- glmnet(xtrain[fold != k, ], ytrain[fold != k], lambda = cv.out$lambda.min, alpha = elastic_net_alpha, standardize = TRUE)
pred <- predict(fit, newx = xtrain[fold == k, ])
} else if (model == "svm_linear") {
fit <- svm(y ~ ., data = train_data_pca, type = "eps-regression", kernel = "linear")
pred <- predict(fit, newdata = val_data_pca)
} else if (model == "svm_radial") {
fit <- svm(y ~ ., data = train_data_pca, type = "eps-regression", kernel = "radial")
pred <- predict(fit, newdata = val_data_pca)
} else if (model == "knn") {
pred <- knn(train = train_data_pca[, -1], test = val_data_pca[, -1], cl = train_data_pca$y, k = k_neighbors)
} else if (model == "tree") {
# Entraîner un modèle d'arbre de décision
fit <- rpart(y ~ ., data = train_data_pca)
pred <- predict(fit, newdata = val_data_pca)
} else if (model == "random_forest") {
fit <- randomForest(y ~ ., data = train_data_pca)
pred <- predict(fit, newdata = val_data_pca)
}
# Calculate specified metric for the validation fold
metric.val[k] <- calculate_metric(val_data_pca$y, pred, metric)
}
# Return mean and standard deviation of the selected metric across folds
c(mean(metric.val), sd(metric.val))
}
summary_metric_models(data, "RMSE")
estimate_regression_metrics <- function(model, data.train, metric = "RMSE", pca_components = 0, elastic_net_alpha = 0.5, k_neighbors = 15) {
# Number of folds
K <- 10
set.seed(20240811)
# Create fold assignments
fold <- sample(1:K, nrow(data.train), replace = TRUE)
# Initialize metric values
metric.val <- rep(0, K)
# Extract response variable and predictor matrix
ytrain <- data.train$y
xtrain <- model.matrix(y ~ . - 1, data.train)  # Exclude intercept
for (k in 1:K) {
# Split data into training and validation sets
train_data <- data.train[fold != k, ]
val_data <- data.train[fold == k, ]
# If PCA is required, preprocess with PCA on training data only
if (pca_components != 0) {
pca <- prcomp(train_data[, -which(names(train_data) == "y")], center = TRUE, scale. = TRUE)
xtrain_pca <- predict(pca, train_data[, -which(names(train_data) == "y")])
xval_pca <- predict(pca, val_data[, -which(names(val_data) == "y")])
# Select the specified number of components
xtrain_pca <- xtrain_pca[, 1:pca_components]
xval_pca <- xval_pca[, 1:pca_components]
# Update training and validation sets with PCA components
train_data_pca <- data.frame(y = train_data$y, xtrain_pca)
val_data_pca <- data.frame(y = val_data$y, xval_pca)
} else {
train_data_pca <- train_data
val_data_pca <- val_data
}
# Model fitting and prediction
if (model == "lm") {
fit <- lm(y ~ ., data = train_data_pca)
pred <- predict(fit, newdata = val_data_pca)
} else if (model == "ridge") {
cv.out <- cv.glmnet(xtrain[fold != k, ], ytrain[fold != k], alpha = 0, standardize = TRUE)
fit <- glmnet(xtrain[fold != k, ], ytrain[fold != k], lambda = cv.out$lambda.min, alpha = 0, standardize = TRUE)
pred <- predict(fit, newx = xtrain[fold == k, ])
} else if (model == "lasso") {
cv.out <- cv.glmnet(xtrain[fold != k, ], ytrain[fold != k], alpha = 1, standardize = TRUE)
fit <- glmnet(xtrain[fold != k, ], ytrain[fold != k], lambda = cv.out$lambda.min, alpha = 1, standardize = TRUE)
pred <- predict(fit, newx = xtrain[fold == k, ])
} else if (model == "gam") {
predictor_names <- names(train_data_pca)[-which(names(train_data_pca) == "y")]
formula_str <- paste("y ~", paste(predictor_names, collapse = " + "))
fit <- gam(as.formula(formula_str), data = train_data_pca, family = gaussian())
pred <- predict(fit, newdata = val_data_pca)
} else if (model == "elastic_net") {
cv.out <- cv.glmnet(xtrain[fold != k, ], ytrain[fold != k], alpha = elastic_net_alpha, standardize = TRUE)
fit <- glmnet(xtrain[fold != k, ], ytrain[fold != k], lambda = cv.out$lambda.min, alpha = elastic_net_alpha, standardize = TRUE)
pred <- predict(fit, newx = xtrain[fold == k, ])
} else if (model == "svm_linear") {
fit <- svm(y ~ ., data = train_data_pca, type = "eps-regression", kernel = "linear")
pred <- predict(fit, newdata = val_data_pca)
} else if (model == "svm_radial") {
fit <- svm(y ~ ., data = train_data_pca, type = "eps-regression", kernel = "radial")
pred <- predict(fit, newdata = val_data_pca)
} else if (model == "knn") {
pred <- knn(train = train_data_pca[, -1], test = val_data_pca[, -1], cl = train_data_pca$y, k = k_neighbors)
} else if (model == "tree") {
# Entraîner un modèle d'arbre de décision
fit <- rpart(y ~ ., data = train_data_pca)
pred <- predict(fit, newdata = val_data_pca)
} else if (model == "random_forest") {
fit <- randomForest(y ~ ., data = train_data_pca)
pred <- predict(fit, newdata = val_data_pca)
}
# Calculate specified metric for the validation fold
metric.val[k] <- calculate_metric(val_data_pca$y, pred, metric)
}
# Return mean and standard deviation of the selected metric across folds
c(mean(metric.val), sd(metric.val))
}
summary_metric_models <- function(data, metric = "RMSE", components_list_models = rep(NULL, 10)) {
print("COMPARAISON DES MODELES DE REGRESSION")
print(metric)
models <- c("lm", "lasso", "ridge", "gam", "elastic_net", "svm_linear", "svm_radial", "knn", "tree", "random_forest")
for (model in models) {
result <- estimate_regression_metrics(model, data, metric = metric, elastic_net_alpha = 0.6)
print(paste(model, ":", metric, "= Mean =", result[1], ", Std Dev =", result[2]))
}
}
summary_metric_models(data, "RMSE")
data$y
X1
data$y
alpha_values <- seq(0, 1, by = 0.1)
rmse_means <- numeric(length(alpha_values))
rmse_stds <- numeric(length(alpha_values))
for (i in seq_along(alpha_values)) {
alpha <- alpha_values[i]
rmse <- estimate_rmse("elastic_net", data.train, elastic_net_alpha = alpha)
rmse_means[i] <- rmse[1]
rmse_stds[i] <- rmse[2]
}
alpha_values <- seq(0, 1, by = 0.1)
rmse_means <- numeric(length(alpha_values))
rmse_stds <- numeric(length(alpha_values))
for (i in seq_along(alpha_values)) {
alpha <- alpha_values[i]
rmse <- estimate_regression_metrics("elastic_net", data.train, elastic_net_alpha = alpha)
rmse_means[i] <- rmse[1]
rmse_stds[i] <- rmse[2]
}
alpha_values <- seq(0, 1, by = 0.1)
rmse_means <- numeric(length(alpha_values))
rmse_stds <- numeric(length(alpha_values))
for (i in seq_along(alpha_values)) {
alpha <- alpha_values[i]
rmse <- estimate_regression_metrics("elastic_net", data, elastic_net_alpha = alpha)
rmse_means[i] <- rmse[1]
rmse_stds[i] <- rmse[2]
}
plot(alpha_values, rmse_means, type = "l", main = "Choix du alpha pour Elastic Net",
xlab = "Alpha", ylab = "RMSE", col = "blue", lwd = 2)
lines(alpha_values, rmse_means + rmse_stds, col = "lightblue", lty = 2)
lines(alpha_values, rmse_means - rmse_stds, col = "lightblue", lty = 2)
min_rmse <- min(rmse_means)
optimal_alpha_index <- which.min(rmse_means)
optimal_alpha <- alpha_values[optimal_alpha_index]
abline(v = optimal_alpha, col = "red", lty = 2)
rmse_means
rmse_stds
plot(alpha_values, rmse_means, type = "l", main = "Choix du alpha pour Elastic Net",
xlab = "Alpha", ylab = "RMSE", col = "blue", lwd = 1)
lines(alpha_values, rmse_means + rmse_stds, col = "lightgreen", lty = 1)
lines(alpha_values, rmse_means - rmse_stds, col = "lightgreen", lty = 1)
min_rmse <- min(rmse_means)
optimal_alpha_index <- which.min(rmse_means)
optimal_alpha <- alpha_values[optimal_alpha_index]
abline(v = optimal_alpha, col = "red", lty = 2)
components.lm <- plot_pca_components_model("lm", data, metric = "rmse")
plot_pca_components_model <- function(model, data, metric = "RMSE"){
list_components <- 2:100
list_avg_metric <- rep(0, 99)
list_std_metric <- rep(0, 99)
for (n_components in list_components){
temp <- estimate_metric(model, data, metric, n_components = n_components)
list_avg_metric[n_components - 1] <- temp[1]
list_std_metric[n_components - 1] <- temp[2]
}
plot(2:100, list_avg_metric, type = "l", main = paste("PCA", metric, "results with", model), xlab = "Nombre de composantes", ylab = metric)
min_metric_val <- min(list_avg_metric)
min_index <- which.min(list_avg_metric) + 1
abline(v = min_index, col = "red", lty = 2)
print(sprintf("Le nombre de composantes optimal est : %d", min_index))
return(min_index)
}
components.lm <- plot_pca_components_model("lm", data, metric = "rmse")
plot_pca_components_model <- function(model, data, metric = "RMSE"){
list_components <- 2:100
list_avg_metric <- rep(0, 99)
list_std_metric <- rep(0, 99)
for (n_components in list_components){
temp <- estimate_regression_metrics(model, data, metric, n_components = n_components)
list_avg_metric[n_components - 1] <- temp[1]
list_std_metric[n_components - 1] <- temp[2]
}
plot(2:100, list_avg_metric, type = "l", main = paste("PCA", metric, "results with", model), xlab = "Nombre de composantes", ylab = metric)
min_metric_val <- min(list_avg_metric)
min_index <- which.min(list_avg_metric) + 1
abline(v = min_index, col = "red", lty = 2)
print(sprintf("Le nombre de composantes optimal est : %d", min_index))
return(min_index)
}
components.lm <- plot_pca_components_model("lm", data, metric = "rmse")
estimate_regression_metrics <- function(model, data.train, metric = "RMSE", n_components = 0, elastic_net_alpha = 0.5, k_neighbors = 15) {
# Number of folds
K <- 10
set.seed(20240811)
# Create fold assignments
fold <- sample(1:K, nrow(data.train), replace = TRUE)
# Initialize metric values
metric.val <- rep(0, K)
# Extract response variable and predictor matrix
ytrain <- data.train$y
xtrain <- model.matrix(y ~ . - 1, data.train)  # Exclude intercept
for (k in 1:K) {
# Split data into training and validation sets
train_data <- data.train[fold != k, ]
val_data <- data.train[fold == k, ]
# If PCA is required, preprocess with PCA on training data only
if (n_components != 0) {
pca <- prcomp(train_data[, -which(names(train_data) == "y")], center = TRUE, scale. = TRUE)
xtrain_pca <- predict(pca, train_data[, -which(names(train_data) == "y")])
xval_pca <- predict(pca, val_data[, -which(names(val_data) == "y")])
# Select the specified number of components
xtrain_pca <- xtrain_pca[, 1:n_components]
xval_pca <- xval_pca[, 1:n_components]
# Update training and validation sets with PCA components
train_data_pca <- data.frame(y = train_data$y, xtrain_pca)
val_data_pca <- data.frame(y = val_data$y, xval_pca)
} else {
train_data_pca <- train_data
val_data_pca <- val_data
}
# Model fitting and prediction
if (model == "lm") {
fit <- lm(y ~ ., data = train_data_pca)
pred <- predict(fit, newdata = val_data_pca)
} else if (model == "ridge") {
cv.out <- cv.glmnet(xtrain[fold != k, ], ytrain[fold != k], alpha = 0, standardize = TRUE)
fit <- glmnet(xtrain[fold != k, ], ytrain[fold != k], lambda = cv.out$lambda.min, alpha = 0, standardize = TRUE)
pred <- predict(fit, newx = xtrain[fold == k, ])
} else if (model == "lasso") {
cv.out <- cv.glmnet(xtrain[fold != k, ], ytrain[fold != k], alpha = 1, standardize = TRUE)
fit <- glmnet(xtrain[fold != k, ], ytrain[fold != k], lambda = cv.out$lambda.min, alpha = 1, standardize = TRUE)
pred <- predict(fit, newx = xtrain[fold == k, ])
} else if (model == "gam") {
predictor_names <- names(train_data_pca)[-which(names(train_data_pca) == "y")]
formula_str <- paste("y ~", paste(predictor_names, collapse = " + "))
fit <- gam(as.formula(formula_str), data = train_data_pca, family = gaussian())
pred <- predict(fit, newdata = val_data_pca)
} else if (model == "elastic_net") {
cv.out <- cv.glmnet(xtrain[fold != k, ], ytrain[fold != k], alpha = elastic_net_alpha, standardize = TRUE)
fit <- glmnet(xtrain[fold != k, ], ytrain[fold != k], lambda = cv.out$lambda.min, alpha = elastic_net_alpha, standardize = TRUE)
pred <- predict(fit, newx = xtrain[fold == k, ])
} else if (model == "svm_linear") {
fit <- svm(y ~ ., data = train_data_pca, type = "eps-regression", kernel = "linear")
pred <- predict(fit, newdata = val_data_pca)
} else if (model == "svm_radial") {
fit <- svm(y ~ ., data = train_data_pca, type = "eps-regression", kernel = "radial")
pred <- predict(fit, newdata = val_data_pca)
} else if (model == "knn") {
pred <- knn(train = train_data_pca[, -1], test = val_data_pca[, -1], cl = train_data_pca$y, k = k_neighbors)
} else if (model == "tree") {
# Entraîner un modèle d'arbre de décision
fit <- rpart(y ~ ., data = train_data_pca)
pred <- predict(fit, newdata = val_data_pca)
} else if (model == "random_forest") {
fit <- randomForest(y ~ ., data = train_data_pca)
pred <- predict(fit, newdata = val_data_pca)
}
# Calculate specified metric for the validation fold
metric.val[k] <- calculate_metric(val_data_pca$y, pred, metric)
}
# Return mean and standard deviation of the selected metric across folds
c(mean(metric.val), sd(metric.val))
}
components.lm <- plot_pca_components_model("lm", data, metric = "rmse")
components.lm <- plot_pca_components_model("lm", data, metric = "RMSE")
components.ridge <- plot_pca_components_model("ridge", data, metric = "RMSE") # Best =
components.lasso <- plot_pca_components_model("lasso", data, metric = "RMSE") # Best =
list_avg_metric
estimate_regression_metrics("ridge", data, metric = "RMSE")
estimate_regression_metrics("ridge", data, metric = "RMSE", n_components = 10)
estimate_regression_metrics <- function(model, data.train, metric = "RMSE", n_components = 0, elastic_net_alpha = 0.5, k_neighbors = 15) {
# Number of folds
K <- 10
set.seed(20240811)
# Create fold assignments
fold <- sample(1:K, nrow(data.train), replace = TRUE)
# Initialize metric values
metric.val <- rep(0, K)
# Extract response variable and predictor matrix
ytrain <- data.train$y
xtrain <- model.matrix(y ~ . - 1, data.train)  # Exclude intercept
for (k in 1:K) {
# Split data into training and validation sets
train_data <- data.train[fold != k, ]
val_data <- data.train[fold == k, ]
# If PCA is required, preprocess with PCA on training data only
if (n_components != 0) {
pca <- prcomp(train_data[, -which(names(train_data) == "y")], center = TRUE, scale. = TRUE)
xtrain_pca <- predict(pca, train_data[, -which(names(train_data) == "y")])
xval_pca <- predict(pca, val_data[, -which(names(val_data) == "y")])
# Select the specified number of components
xtrain_pca <- xtrain_pca[, 1:n_components]
xval_pca <- xval_pca[, 1:n_components]
# Update training and validation sets with PCA components
train_data_pca <- data.frame(y = train_data$y, xtrain_pca)
val_data_pca <- data.frame(y = val_data$y, xval_pca)
} else {
train_data_pca <- train_data
val_data_pca <- val_data
}
# Model fitting and prediction
if (model == "lm") {
fit <- lm(y ~ ., data = train_data_pca)
pred <- predict(fit, newdata = val_data_pca)
} else if (model == "ridge") {
xtrain <- model.matrix(y ~ . - 1, train_data_pca)
xval <- model.matrix(y ~ . - 1, val_data_pca)
cv.out <- cv.glmnet(xtrain, train_data_pca$y, alpha = 0, standardize = TRUE)
fit <- glmnet(xtrain, train_data_pca$y, lambda = cv.out$lambda.min, alpha = 0, standardize = TRUE)
pred <- predict(fit, newx = xval)
} else if (model == "lasso") {
xtrain <- model.matrix(y ~ . - 1, train_data_pca)
xval <- model.matrix(y ~ . - 1, val_data_pca)
cv.out <- cv.glmnet(xtrain, train_data_pca$y, alpha = 1, standardize = TRUE)
fit <- glmnet(xtrain, train_data_pca$y, lambda = cv.out$lambda.min, alpha = 1, standardize = TRUE)
pred <- predict(fit, newx = xval)
} else if (model == "gam") {
predictor_names <- names(train_data_pca)[-which(names(train_data_pca) == "y")]
formula_str <- paste("y ~", paste(predictor_names, collapse = " + "))
fit <- gam(as.formula(formula_str), data = train_data_pca, family = gaussian())
pred <- predict(fit, newdata = val_data_pca)
} else if (model == "elastic_net") {
cv.out <- cv.glmnet(xtrain[fold != k, ], ytrain[fold != k], alpha = elastic_net_alpha, standardize = TRUE)
fit <- glmnet(xtrain[fold != k, ], ytrain[fold != k], lambda = cv.out$lambda.min, alpha = elastic_net_alpha, standardize = TRUE)
pred <- predict(fit, newx = xtrain[fold == k, ])
} else if (model == "svm_linear") {
fit <- svm(y ~ ., data = train_data_pca, type = "eps-regression", kernel = "linear")
pred <- predict(fit, newdata = val_data_pca)
} else if (model == "svm_radial") {
fit <- svm(y ~ ., data = train_data_pca, type = "eps-regression", kernel = "radial")
pred <- predict(fit, newdata = val_data_pca)
} else if (model == "knn") {
pred <- knn(train = train_data_pca[, -1], test = val_data_pca[, -1], cl = train_data_pca$y, k = k_neighbors)
} else if (model == "tree") {
# Entraîner un modèle d'arbre de décision
fit <- rpart(y ~ ., data = train_data_pca)
pred <- predict(fit, newdata = val_data_pca)
} else if (model == "random_forest") {
fit <- randomForest(y ~ ., data = train_data_pca)
pred <- predict(fit, newdata = val_data_pca)
}
# Calculate specified metric for the validation fold
metric.val[k] <- calculate_metric(val_data_pca$y, pred, metric)
}
# Return mean and standard deviation of the selected metric across folds
c(mean(metric.val), sd(metric.val))
}
estimate_regression_metrics("ridge", data, metric = "RMSE", n_components = 10)
estimate_regression_metrics("ridge", data, metric = "RMSE", n_components = 100)
components.ridge <- plot_pca_components_model("ridge", data, metric = "RMSE") # Best = 100
components.lasso <- plot_pca_components_model("lasso", data, metric = "RMSE") # Best =
components.elastic_net <- plot_pca_components_model("elastic_net", data, metric = "RMSE") # Best = 100
